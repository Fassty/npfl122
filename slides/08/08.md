title: NPFL122, Lecture 8
class: title, langtech, cc-by-nc-sa
# TD3, SAC, TRPO, PPO

## Milan Straka

### November 23, 2020

---
section: TD3
# Twin Delayed Deep Deterministic Policy Gradient

The paper Addressing Function Approximation Error in Actor-Critic Methods by
Scott Fujimoto et al. from February 2018 proposes improvements to DDPG which

~~~
- decrease maximization bias by training two critics and choosing minimum of
  their predictions;

~~~
- introduce several variance-lowering optimizations:
  - delayed policy updates;
  - target policy smoothing.

~~~

The TD3 algorithm has been together with SAC one of the state-of-the-art
algorithms for off-policy continuous-actions RL training (as of 2020).

---
# TD3 – Maximization Bias

Similarly to Q-learning, the DDPG algorithm suffers from maximization bias.
In Q-learning, the maximization bias was caused by the explicit $\max$ operator.
For DDPG methods, it can be caused by the gradient descent itself. Let
$→θ_\textit{approx}$ be the parameters maximizing the $q_→θ$ and let
$→θ_\textit{true}$ be the hypothetical parameters which maximise true $q_π$,
and let $π_\textit{approx}$ and $π_\textit{true}$ denote the corresponding
policies.

~~~
Because the gradient direction is a local maximizer, for sufficiently small
$α<ε_1$ we have
$$𝔼\big[q_→θ(s, π_\textit{approx})\big] ≥ 𝔼\big[q_→θ(s, π_\textit{true})\big].$$

~~~
However, for real $q_π$ and for sufficiently small $α<ε_2$ it holds that
$$𝔼\big[q_π(s, π_\textit{true})\big] ≥ 𝔼\big[q_π(s, π_\textit{approx})\big].$$

~~~
Therefore, if $𝔼\big[q_→θ(s, π_\textit{true})\big] ≥ 𝔼\big[q_π(s, π_\textit{true})\big]$,
for $α < \min(ε_1, ε_2)$
$$𝔼\big[q_→θ(s, π_\textit{approx})\big] ≥ 𝔼\big[q_π(s, π_\textit{approx})\big].$$

---
# TD3 – Maximization Bias

![w=50%](td3_bias.svgz)![w=50%](td3_bias_dqac.svgz)

~~~
Analogously to Double DQN we could compute the learning targets using
the current policy and the target critic, i.e., $r + γ q_{→θ'}(s', π_→φ(s'))$
(instead of using target policy and target critic as in DDPG), obtaining DDQN-AC algorithm.
However, the authors found out that the policy changes too slowly and the target
and current networks are too similar.

~~~
Using the original Double Q-learning, two pairs of actors and critics could be
used, with the learning targets computed by the opposite critic, i.e.,
$r + γ q_{→θ_2}(s', π_{→φ_1}(s))$ for updating $q_{→θ_1}$. The resulting DQ-AC
algorithm is slightly better, but still suffering from oversetimation.

---
# TD3 – Algorithm

The authors instead suggest to employ two critics and one actor. The actor is
trained using one of the critics, and both critics are trained using the same
target computed using the _minimum_ value of both critics as
$$r + γ \min_{i=1,2} q_{→θ'_i}(s', π_{→φ'}(s')).$$

~~~
Furthermore, the authors suggest two additional improvements for variance
reduction.
- For obtaining higher quality target values, the authors propose to train the
  critics more often. Therefore, critics are updated each step, but the actor
  and the target networks are updated only every $d$-th step ($d=2$ is used in
  the paper).

~~~
- To explictly model that similar actions should lead to similar results,
  a small random noise is added to performed actions when computing the target
  value:
  $$r + γ \min_{i=1,2} q_{→θ'_i}(s', π_{→φ'}(s') + ε)~~~\textrm{for}~~~
    ε ∼ \operatorname{clip}(𝓝(0, σ), -c, c).$$

---
# TD3 – Algorithm

![w=43%,h=center](td3_algorithm.svgz)

---
# TD3 – Algorithm

![w=80%,h=center](td3_hyperparameters.svgz)

---
# TD3 – Results

![w=70%,h=center](td3_results_curves.svgz)
![w=70%,h=center](td3_results.svgz)

---
# TD3 – Ablations

![w=100%,h=center](td3_ablations.svgz)
![w=100%,h=center](td3_ablations_dqac.svgz)

---
# TD3 – Ablations

![w=65%,h=center](td3_ablations_results.svgz)

---
section: SAC
# Soft Actor Critic

The paper Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actor by Tuomas Haarnoja et al. introduces
a different off-policy algorithm for continuous action space.

~~~
The general idea is to introduce entropy directly in the value function we want
to maximize.

---
# Soft Actor Critic
![w=60%,h=center](sac_algorithm.svgz)

---
# Soft Actor Critic
![w=90%](sac_results.svgz)

---
section: NPG
# Natural Policy Gradient

The following approach has been introduced by Kakade (2002).

~~~
Using policy gradient theorem, we are able to compute $∇ v_π$. Normally, we
update the parameters by using directly this gradient. This choice is justified
by the fact that a vector $→d$ which maximizes $v_π(s; →θ + →d)$ under
the constraint that $|→d|^2$ is bounded by a small constant is exactly
the gradient $∇ v_π$.

~~~
Normally, the length $|→d|^2$ is computed using Euclidean metric. But in general,
any metric could be used. Representing a metric using a positive-definite matrix
$⇉G$ (identity matrix for Euclidean metric), we can compute the distance as
$|→d|^2 = ∑_{ij} G_{ij} d_i d_j = →d^T ⇉G →d$. The steepest ascent direction is
then given by $⇉G^{-1} ∇ v_π$.

~~~
Note that when $⇉G$ is the Hessian $⇉H v_π$, the above process is exactly
Newton's method.

---
# Natural Policy Gradient

![w=100%,v=middle](npg.svgz)

---
# Natural Policy Gradient

A suitable choice for the metric is _Fisher information matrix_ defined as
$$F_s(→θ) ≝ 𝔼_{π(a | s; →θ)} \left[\frac{∂ \log π(a | s; →θ)}{∂ →θ_i} \frac{∂ \log π(a | s; →θ)}{∂ →θ_j} \right]
\color{gray} = 𝔼[∇ π(a | s; →θ)] 𝔼[∇ π(a | s; →θ)]^T.$$

~~~
It can be shown that the Fisher information metric is the only Riemannian metric
(up to rescaling) invariant to change of parameters under sufficient statistic.

~~~
Recall Kullback-Leibler distance (or relative entropy) defined as
$$D_\textrm{KL}(→p || →q) ≝ ∑_i p_i \log \frac{p_i}{q_i} \color{gray} = H(p, q) - H(p).$$

~~~
The Fisher information matrix is also a Hessian of the
$D_\textrm{KL}(π(a | s; →θ) || π(a | s; →θ')$:
$$F_s(→θ) = \frac{∂^2}{∂θ_i' ∂θ_j'} D_\textrm{KL}(π(a | s; →θ) || π(a | s; →θ')\Big|_{→θ' = →θ}.$$

---
# Natural Policy Gradient

Using the metric
$$F(→θ) = 𝔼_{s ∼ μ_→θ} F_s(→θ)$$
we want to update the parameters using $→d_F ≝ F(→θ)^{-1} ∇ v_π$.

~~~
An interesting property of using the $→d_F$ to update the parameters is that
- updating $→θ$ using $∇ v_π$ will choose an arbitrary _better_ action in state
  $s$;
~~~
- updating $→θ$ using $F(→θ)^{-1} ∇ v_π$ chooses the _best_ action (maximizing
  expected return), similarly to tabular greedy policy improvement.

~~~
However, computing $→d_F$ in a straightforward way is too costly.

---
# Truncated Natural Policy Gradient

Duan et al. (2016) in paper _Benchmarking Deep Reinforcement Learning for
Continuous Control_ propose a modification to the NPG to efficiently compute
$→d_F$.

~~~
Following Schulman et al. (2015), they suggest to use _conjugate gradient
algorithm_, which can solve a system of linear equations $⇉A→x = →b$
in an iterative manner, by using $⇉A$ only to compute products $⇉A→v$ for
a suitable $→v$.

~~~
Therefore, $→d_F$ is found as a solution of
$$F(→θ)→d_F = ∇ v_π$$
and using only 10 iterations of the algorithm seem to suffice according to the
experiments.

~~~
Furthermore, Duan et al. suggest to use a specific learning rate suggested by
Peters et al (2008) of
$$\frac{α}{\sqrt{(∇ v_π)^T F(→θ)^{-1} ∇ v_π}}.$$

---
section: TRPO
# Trust Region Policy Optimization

Schulman et al. in 2015 wrote an influential paper introducing TRPO as an
improved variant of NPG.

~~~
Considering two policies $π, π̃$, we can write
$$v_π̃ = v_π + 𝔼_{s ∼ μ(π̃)} 𝔼_{a ∼ π̃(a | s)} a_π(a | s),$$
where $a_π(a | s)$ is the advantage function $q_π(a | s) - v_π(s)$ and
$μ(π̃)$ is the on-policy distribution of the policy $π̃$.

~~~
Analogously to policy improvement, we see that if $a_π(a | s) ≥0$, policy
$π̃$ performance increases (or stays the same if the advantages are zero
everywhere).

~~~
However, sampling states $s ∼ μ(π̃)$ is costly. Therefore, we instead
consider
$$L_π(π̃) = v_π + 𝔼_{s ∼ μ(π)} 𝔼_{a ∼ π̃(a | s)} a_π(a | s).$$

---
# Trust Region Policy Optimization
$$L_π(π̃) = v_π + 𝔼_{s ∼ μ(π)} 𝔼_{a ∼ π̃(a | s)} a_π(a | s)$$

It can be shown that for parametrized $π(a | s; →θ)$ the $L_π(π̃)$ matches
$v_{π̃}$ to the first order.

~~~
Schulman et al. additionally proves that if we denote
$α = D_\textrm{KL}^\textrm{max}(π_\textrm{old} || π_\textrm{new})
   = \max_s D_\textrm{KL}\big(π_\textrm{old}(⋅|s) || π_\textrm{new}(⋅|s)\big)$, then
$$v_{π_\textrm{new}} ≥ L_{π_\textrm{old}}(π_\textrm{new}) - \frac{4εγ}{(1-γ)^2}α\textrm{~~~where~~~}ε = \max_{s, a} |a_π(s, a)|.$$

~~~
Therefore, TRPO minimizes $L_{π_{→θ_0}}(π_→θ)$ subject to
$D_\textrm{KL}^{→θ_0}(π_{→θ_0} || π_→θ) < δ$, where
- $D_\textrm{KL}^{→θ_0}(π_{→θ_0} || π_→θ) = 𝔼_{s ∼ μ(π_{→θ_0})} [D_\textrm{KL}\big(π_\textrm{old}(⋅|s) || π_\textrm{new}(⋅|s)\big)]$
  is used instead of $D_\textrm{KL}^\textrm{max}$ for performance reasons;
~~~
- $δ$ is a constant found empirically, as the one implied by the above equation
  is too small;
~~~
- importance sampling is used to account for sampling actions from $π$.

---
# Trust Region Policy Optimization

$$\textrm{minimize}~~L_{π_{→θ_0}}(π_→θ)~~\textrm{subject to}~~D_\textrm{KL}^{→θ_0}(π_{→θ_0} || π_→θ) < δ$$

The parameters are updated using $→d_F = F(→θ)^{-1} ∇ L_{π_{→θ_0}}(π_→θ)$, utilizing the
conjugate gradient algorithm as described earlier for TNPG (note that the
algorithm was designed originally for TRPO and only later employed for TNPG).

~~~
To guarantee improvement and respect the $D_\textrm{KL}$ constraint, a line
search is in fact performed. We start by the learning rate of
$\sqrt{δ/(→d_F^T F(→θ)^{-1} →d_F)}$ and shrink it exponentially until
the constraint is satistifed and the objective improves.

---
# Trust Region Policy Optimization

![w=30%,h=center](rllib_tasks.svgz)

![w=100%](rllib_results.svgz)

---
section: PPO
# Proximal Policy Optimization

A simplification of TRPO which can be implemented using a few lines of code.

Let $r_t(→θ) ≝ \frac{π(A_t|S_t; →θ)}{π(A_t|S_t; →θ_\textrm{old})}$. PPO
minimizes the objective
$$L^\textrm{CLIP}(→θ) ≝ 𝔼_t\Big[\min\big(r_t(→θ) Â_t, \operatorname{clip}(r_t(→θ), 1-ε, 1+ε) Â_t)\big)\Big].$$

Such $L^\textrm{CLIP}(→θ)$ is a lower (pessimistic) bound.

![w=60%,h=center](ppo_clipping.svgz)

---
# Proximal Policy Optimization

The advantages $Â_t$ are additionally estimated using _generalized
advantage estimation_ (which we will talk in some future lecture)
instead of the $n$-step definition we used until now,
i.e., $Â_t = ∑_{i=0}^{n-1} γ^i R_{t+1+i} + γ^{n} V(S_{t+n}) - V(S_t)$.

![w=80%,h=center](ppo_algorithm.svgz)

---
# Proximal Policy Optimization

![w=100%,v=middle](ppo_results.svgz)
