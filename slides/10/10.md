title: NPFL122, Lecture 10
class: title, langtech, cc-by-nc-sa
# PopArt Normalization

## Milan Straka

### December 07, 2020

---
section: PopArt Normalization
# PopArt Normalization

An improvement of IMPALA from Sep 2018, which performs normalization of task
rewards instead of just reward clipping. PopArt stands for _Preserving Outputs
Precisely, while Adaptively Rescaling Targets_.

~~~
Assume the value estimate $v(s; →θ, σ, μ)$ is computed using a normalized value
predictor $n(s; →θ)$
$$v(s; →θ, σ, μ) ≝ σ n(s; →θ) + μ$$
and further assume that $n(s; →θ)$ is an output of a linear function
$$n(s; →θ) ≝ →ω^T f(s; →θ-\{→ω, b\}) + b.$$

~~~
We can update the $σ$ and $μ$ using exponentially moving average with decay rate
$β$ (in the paper, first moment $μ$ and second moment $υ$ is tracked, and
standard deviation is computed as $σ=\sqrt{υ-μ^2}$; decay rate $β=3 ⋅ 10^{-4}$ is employed).

---
# PopArt Normalization

Utilizing the parameters $μ$ and $σ$, we can normalize the observed (unnormalized) returns as
$(G - μ) / σ$ and use an actor-critic algorithm with advantage $(G - μ)/σ - n(S; →θ)$.

~~~
However, in order to make sure the value function estimate does not change when
the normalization parameters change, the parameters $→ω, b$ computing the
unnormalized value estimate are updated under any change $μ → μ'$ and $σ → σ'$ as:
$$→ω' ≝ \frac{σ}{σ'}→ω,~~~~b' ≝ \frac{σb + μ - μ'}{σ'}.$$

~~~
In multi-task settings, we train a task-agnostic policy and task-specific value
functions (therefore, $→μ$, $→σ$ and $→n(s; →θ)$ are vectors).

---
# PopArt Results

![w=80%,h=center](popart_results.svgz)

~~~
![w=100%](popart_atari_curves.svgz)

---
# PopArt Results

![w=90%,h=center](popart_atari_statistics.svgz)

---
# PopArt Results

![w=100%,v=middle](popart_dmlab_curves.svgz)
