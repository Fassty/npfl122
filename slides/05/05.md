title: NPFL122, Lecture 5
class: title, langtech, cc-by-nc-sa
# Rainbow

## Milan Straka

### November 02, 2020

---
section: Rainbow
# Rainbow

There have been many suggested improvements to the DQN architecture. In the end
of 2017, the _Rainbow: Combining Improvements in Deep Reinforcement Learning_
paper combines 7 of them into a single architecture they call **Rainbow**.

~~~
![w=38%,h=center](rainbow_results.svgz)

---
section: DDQN
# Q-learning and Maximization Bias

Because behaviour policy in Q-learning is $ε$-greedy variant of the target
policy, the same samples (up to $ε$-greedy) determine both the maximizing action
and estimate its value.

~~~
![w=75%,h=center](../03/double_q_learning_example.svgz)

---
# Double Q-learning

![w=80%,h=center](../03/double_q_learning.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

Similarly to double Q-learning, instead of
$$r + γ \max_{a'} Q(s', a'; θ̄) - Q(s, a; θ),$$
we minimize
$$r + γ Q(s', \argmax_{a'}Q(s', a'; θ); θ̄) - Q(s, a; θ).$$

~~~
![w=30%,h=center](ddqn_errors.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

![w=100%,h=center](ddqn_errors_analysis.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

![w=60%,h=center](ddqn_analysis.svgz)

---
# Rainbow DQN Extensions

## Double Q-learning

Performance on episodes taking at most 5 minutes and no-op starts:
![w=40%,h=center,mh=40%,v=middle](ddqn_results_5min.svgz)

Performance on episodes taking at most 30 minutes and using human starts:
![w=55%,h=center,mh=40%,v=middle](ddqn_results_30min.svgz)

---
section: PriRep
# Rainbow DQN Extensions

## Prioritized Replay

Instead of sampling the transitions uniformly from the replay buffer,
we instead prefer those with a large TD error. Therefore, we sample transitions
according to their probability
$$p_t ∝ \Big|r + γ \max_{a'} Q(s', a'; θ̄) - Q(s, a; θ)\Big|^ω,$$
~~~
where $ω$ controls the shape of the distribution (which is uniform for $ω=0$
and corresponds to TD error for $ω=1$).

~~~
New transitions are inserted into the replay buffer with maximum probability
to support exploration of all encountered transitions.

---
# Rainbow DQN Extensions

## Prioritized Replay

Because we now sample transitions according to $p_t$ instead of uniformly,
on-policy distribution and sampling distribution differ. To compensate, we
therefore utilize importance sampling with ratio
$$ρ_t = \left( \frac{1/N}{p_t} \right) ^β.$$

~~~
The authors utilize in fact “for stability reasons”
$$ρ_t / \max_i ρ_i.$$

---
# Rainbow DQN Extensions

## Prioritized Replay

![w=75%,h=center](prioritized_dqn_algorithm.svgz)
